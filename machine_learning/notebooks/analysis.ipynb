"""
ğŸ““ AnÃ¡lise de Machine Learning - Notebook em Python
Sistema de ManutenÃ§Ã£o Preditiva - Hermes Reply Challenge

Este arquivo substitui o Jupyter Notebook e contÃ©m toda a anÃ¡lise de ML
"""

# ============================================================================
# ğŸ“š IMPORTAÃ‡Ã•ES E CONFIGURAÃ‡Ã•ES
# ============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print("ğŸš€ SISTEMA DE MANUTENÃ‡ÃƒO PREDITIVA - HERMES REPLY")
print("="*60)
print("ğŸ““ AnÃ¡lise completa de Machine Learning")
print("ğŸ¤– Algoritmo: Random Forest Classifier")
print("ğŸ“Š Objetivo: ClassificaÃ§Ã£o de status de equipamentos")
print("="*60)

# ============================================================================
# ğŸ“Š CARREGAMENTO E EXPLORAÃ‡ÃƒO DOS DADOS
# ============================================================================

def load_and_explore_data():
    """Carrega e explora o dataset"""
    print("\nğŸ“Š CARREGANDO DADOS...")
    
    # Carregando dados
    df = pd.read_csv('../data/sensor_data_sample.csv')
    
    print(f"âœ… Dataset carregado: {len(df)} registros")
    print(f"ğŸ“ˆ Colunas: {list(df.columns)}")
    print(f"ğŸ” Shape: {df.shape}")
    
    # InformaÃ§Ãµes gerais
    print(f"\nğŸ“‹ INFORMAÃ‡Ã•ES GERAIS:")
    print(f"â€¢ Total de registros: {len(df):,}")
    print(f"â€¢ Sensores Ãºnicos: {df['sensor_id'].nunique()}")
    print(f"â€¢ Equipamentos Ãºnicos: {df['equipment_id'].nunique()}")
    print(f"â€¢ PerÃ­odo: {df['timestamp'].min()} atÃ© {df['timestamp'].max()}")
    
    # DistribuiÃ§Ã£o dos status
    print(f"\nğŸ“Š DISTRIBUIÃ‡ÃƒO DOS STATUS:")
    status_counts = df['status'].value_counts()
    for status, count in status_counts.items():
        percentage = (count / len(df)) * 100
        print(f"â€¢ {status}: {count:,} ({percentage:.1f}%)")
    
    # EstatÃ­sticas dos sensores
    numeric_cols = ['temperature', 'humidity', 'pressure', 'vibration', 'current']
    print(f"\nğŸŒ¡ï¸ ESTATÃSTICAS DOS SENSORES:")
    print(df[numeric_cols].describe().round(2))
    
    return df

# ============================================================================
# ğŸ”§ PRÃ‰-PROCESSAMENTO
# ============================================================================

def preprocess_data(df):
    """Prepara os dados para ML"""
    print(f"\nğŸ”§ PRÃ‰-PROCESSAMENTO DOS DADOS...")
    
    # Features e target
    feature_names = ['temperature', 'humidity', 'pressure', 'vibration', 'current']
    X = df[feature_names]
    y = df['status']
    
    # Codificando target
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)
    
    print(f"â€¢ Features: {feature_names}")
    print(f"â€¢ Classes: {list(label_encoder.classes_)}")
    print(f"â€¢ CodificaÃ§Ã£o: {dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))}")
    
    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
    )
    
    print(f"\nğŸ“Š DIVISÃƒO DOS DADOS:")
    print(f"â€¢ Treino: {len(X_train)} amostras ({len(X_train)/len(X)*100:.1f}%)")
    print(f"â€¢ Teste: {len(X_test)} amostras ({len(X_test)/len(X)*100:.1f}%)")
    
    # NormalizaÃ§Ã£o
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    print(f"âœ… Dados normalizados com StandardScaler")
    
    return X_train_scaled, X_test_scaled, y_train, y_test, label_encoder, feature_names

# ============================================================================
# ğŸ¤– TREINAMENTO DO MODELO
# ============================================================================

def train_model(X_train_scaled, y_train):
    """Treina o modelo Random Forest"""
    print(f"\nğŸ¤– TREINANDO MODELO RANDOM FOREST...")
    
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42
    )
    
    # Treinamento
    model.fit(X_train_scaled, y_train)
    
    print(f"âœ… Modelo treinado com sucesso!")
    print(f"ğŸ¯ ParÃ¢metros: n_estimators={model.n_estimators}, max_depth={model.max_depth}")
    
    return model

# ============================================================================
# ğŸ“Š AVALIAÃ‡ÃƒO DO MODELO
# ============================================================================

def evaluate_model(model, X_test_scaled, y_test, label_encoder):
    """Avalia a performance do modelo"""
    print(f"\nğŸ“Š AVALIAÃ‡ÃƒO DO MODELO")
    print("="*50)
    
    # PrediÃ§Ãµes
    y_pred = model.predict(X_test_scaled)
    
    # AcurÃ¡cia
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ğŸ¯ AcurÃ¡cia Geral: {accuracy:.3f} ({accuracy*100:.1f}%)")
    
    # RelatÃ³rio detalhado
    target_names = label_encoder.classes_
    report = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)
    
    print(f"\nğŸ“‹ RELATÃ“RIO POR CLASSE:")
    for class_name in target_names:
        metrics = report[class_name]
        print(f"\nâ€¢ {class_name}:")
        print(f"  - PrecisÃ£o: {metrics['precision']:.3f} ({metrics['precision']*100:.1f}%)")
        print(f"  - Recall: {metrics['recall']:.3f} ({metrics['recall']*100:.1f}%)")
        print(f"  - F1-Score: {metrics['f1-score']:.3f}")
        print(f"  - Suporte: {int(metrics['support'])} amostras")
    
    # Matriz de confusÃ£o
    cm = confusion_matrix(y_test, y_pred)
    print(f"\nğŸ“Š MATRIZ DE CONFUSÃƒO:")
    print("    ", "  ".join(f"{name:>8}" for name in target_names))
    for i, row in enumerate(cm):
        print(f"{target_names[i]:>8}", "  ".join(f"{val:>8}" for val in row))
    
    return accuracy, report, y_pred

# ============================================================================
# ğŸ” IMPORTÃ‚NCIA DAS FEATURES
# ============================================================================

def analyze_feature_importance(model, feature_names):
    """Analisa a importÃ¢ncia das features"""
    print(f"\nğŸ” IMPORTÃ‚NCIA DAS FEATURES")
    print("="*40)
    
    importances = model.feature_importances_
    indices = np.argsort(importances)[::-1]
    
    print(f"ğŸ“Š Ranking de ImportÃ¢ncia:")
    for i, idx in enumerate(indices):
        feature = feature_names[idx]
        importance = importances[idx]
        print(f"{i+1}. {feature}: {importance:.3f} ({importance*100:.1f}%)")
    
    print(f"\nğŸ’¡ Feature mais importante: {feature_names[indices[0]]} ({importances[indices[0]]*100:.1f}%)")
    
    return importances, indices

# ============================================================================
# ğŸ”® EXEMPLOS DE PREDIÃ‡Ã•ES
# ============================================================================

def generate_predictions_examples(model, X_test, X_test_scaled, y_test, label_encoder):
    """Gera exemplos de prediÃ§Ãµes"""
    print(f"\nğŸ”® EXEMPLOS DE PREDIÃ‡Ã•ES EM TEMPO REAL")
    print("="*60)
    
    # Selecionando 3 amostras
    sample_indices = np.random.choice(len(X_test), 3, replace=False)
    
    for i, idx in enumerate(sample_indices):
        real_features = X_test.iloc[idx]
        real_status = label_encoder.classes_[y_test[idx]]
        
        # PrediÃ§Ã£o
        pred_features = X_test_scaled[idx].reshape(1, -1)
        pred_status_encoded = model.predict(pred_features)[0]
        pred_status = label_encoder.classes_[pred_status_encoded]
        
        # Probabilidades
        probabilities = model.predict_proba(pred_features)[0]
        confidence = max(probabilities)
        
        print(f"\nğŸ”§ Equipamento {i+1}:")
        print(f"ğŸ“Š Sensores: T={real_features['temperature']:.1f}Â°C, "
              f"H={real_features['humidity']:.1f}%, "
              f"P={real_features['pressure']:.3f}bar, "
              f"V={real_features['vibration']:.2f}Hz, "
              f"I={real_features['current']:.1f}A")
        print(f"ğŸ¯ Real: {real_status} | Predito: {pred_status} | ConfianÃ§a: {confidence:.1%}")
        
        if real_status == pred_status:
            print("âœ… CORRETO")
        else:
            print("âŒ INCORRETO")

# ============================================================================
# ğŸ† RESUMO FINAL
# ============================================================================

def final_summary(accuracy, feature_names, indices, importances):
    """Resumo final do projeto"""
    print(f"\n" + "="*60)
    print("ğŸ† RESUMO FINAL DO PROJETO")
    print("="*60)
    
    print(f"\nğŸ¤– MODELO DESENVOLVIDO:")
    print(f"â€¢ Algoritmo: Random Forest Classifier")
    print(f"â€¢ AcurÃ¡cia: {accuracy:.1%}")
    print(f"â€¢ Features: {len(feature_names)} sensores")
    print(f"â€¢ Classes: 3 status (NORMAL, ATENÃ‡ÃƒO, CRÃTICO)")
    
    print(f"\nğŸ¯ PROBLEMA RESOLVIDO:")
    print(f"â€¢ ClassificaÃ§Ã£o automÃ¡tica de status de equipamentos")
    print(f"â€¢ DetecÃ§Ã£o precoce de problemas")
    print(f"â€¢ ManutenÃ§Ã£o preditiva eficiente")
    
    print(f"\nï¿½ï¿½ FEATURE MAIS IMPORTANTE:")
    print(f"â€¢ {feature_names[indices[0]]}: {importances[indices[0]]*100:.1f}%")
    
    print(f"\nâœ… PROJETO CONCLUÃDO COM SUCESSO!")
    print(f"ğŸš€ Modelo pronto para implementaÃ§Ã£o industrial!")

# ============================================================================
# ğŸš€ EXECUÃ‡ÃƒO PRINCIPAL
# ============================================================================

if __name__ == "__main__":
    # Executando todo o pipeline
    df = load_and_explore_data()
    X_train_scaled, X_test_scaled, y_train, y_test, label_encoder, feature_names = preprocess_data(df)
    model = train_model(X_train_scaled, y_train)
    accuracy, report, y_pred = evaluate_model(model, X_test_scaled, y_test, label_encoder)
    importances, indices = analyze_feature_importance(model, feature_names)
    generate_predictions_examples(model, pd.DataFrame(X_test_scaled), X_test_scaled, y_test, label_encoder)
    final_summary(accuracy, feature_names, indices, importances)
